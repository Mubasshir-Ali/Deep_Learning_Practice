{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "33_NLP_Constructing_Text_Generation_Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNK7T54znErKdaZ4tJTCAKG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mubasshir-Ali/Deep_Learning_Practice/blob/master/33_NLP_Constructing_Text_Generation_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ERrflIKftm",
        "colab_type": "text"
      },
      "source": [
        "# Constructing A Text Generation Model\n",
        "\n",
        "To practice this method, we'll use the Kaggle Song Lyrics Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqwTspYCKlMP",
        "colab_type": "text"
      },
      "source": [
        "# Import TensorFlow And Related Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msmo72FeKUNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5olWgJNP5wVB",
        "colab_type": "text"
      },
      "source": [
        "# Get The Dataset\n",
        "As noted above, we'll utilize the Song Lyrics dataset on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06U5Fnrp5nGC",
        "colab_type": "code",
        "outputId": "65494d0b-8ab9-4513-f69a-99f8bcdaa89d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-09 05:28:49--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 108.177.119.139, 108.177.119.102, 108.177.119.100, ...\n",
            "Connecting to drive.google.com (drive.google.com)|108.177.119.139|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4oh2794qjpaa0kdk1di6lhtdv04tkqrb/1589002125000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-05-09 05:28:52--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/4oh2794qjpaa0kdk1di6lhtdv04tkqrb/1589002125000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 108.177.119.132, 2a00:1450:4013:c00::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|108.177.119.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv       [    <=>             ]  69.08M  99.9MB/s    in 0.7s    \n",
            "\n",
            "2020-05-09 05:28:53 (99.9 MB/s) - ‘/tmp/songdata.csv’ saved [72436445]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLF5buDh6FIp",
        "colab_type": "text"
      },
      "source": [
        "# First 10 Songs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPIQiXjd6PAd",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZB1IUa3T6ESX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize_corpus(corpus, num_words = -1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenier(num_words = num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3pEEJPmf8CjO",
        "colab_type": "code",
        "outputId": "ba1fb3a5-d802-4b28-a9ff-071976fb50ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype = str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W15o1B8I-Isy",
        "colab_type": "text"
      },
      "source": [
        "# Create Sequences and Labels\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with texts_to_sequences, but also including the use of N-Grams; creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJAAIUaP8yg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "  token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "  for i in range(1, len(token_list)):\n",
        "    n_gram_sequence = token_list[:i+1]\n",
        "    sequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes = total_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJVmoZG0_6-p",
        "colab_type": "code",
        "outputId": "7d557991-3131-487d-a1f0-83ad3a1fef69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 823
        }
      },
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w7JXFUNHA3XX",
        "colab_type": "text"
      },
      "source": [
        "# Train A Text Generation Model\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using more epochs than before, as text generation can take a little longer to converge than sentiment analysis, and we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z8RPIwAyA1Ji",
        "colab_type": "code",
        "outputId": "d1f8a807-67b5-4f53-9133-b540b6401030",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length = max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(20)))\n",
        "model.add(Dense(total_words, activation = 'softmax'))\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs = 200, verbose = 1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 6.0118 - accuracy: 0.0212\n",
            "Epoch 2/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.4463 - accuracy: 0.0394\n",
            "Epoch 3/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.3718 - accuracy: 0.0399\n",
            "Epoch 4/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.3168 - accuracy: 0.0464\n",
            "Epoch 5/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.2426 - accuracy: 0.0464\n",
            "Epoch 6/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.1687 - accuracy: 0.0439\n",
            "Epoch 7/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0935 - accuracy: 0.0454\n",
            "Epoch 8/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 5.0174 - accuracy: 0.0464\n",
            "Epoch 9/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.9416 - accuracy: 0.0530\n",
            "Epoch 10/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.8629 - accuracy: 0.0560\n",
            "Epoch 11/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.7826 - accuracy: 0.0545\n",
            "Epoch 12/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.7035 - accuracy: 0.0621\n",
            "Epoch 13/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 4.6186 - accuracy: 0.0747\n",
            "Epoch 14/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.5425 - accuracy: 0.0858\n",
            "Epoch 15/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4748 - accuracy: 0.0913\n",
            "Epoch 16/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.4145 - accuracy: 0.1009\n",
            "Epoch 17/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.3318 - accuracy: 0.1130\n",
            "Epoch 18/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.2806 - accuracy: 0.1171\n",
            "Epoch 19/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1953 - accuracy: 0.1332\n",
            "Epoch 20/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.1303 - accuracy: 0.1398\n",
            "Epoch 21/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.0749 - accuracy: 0.1524\n",
            "Epoch 22/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 4.0102 - accuracy: 0.1625\n",
            "Epoch 23/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.9444 - accuracy: 0.1761\n",
            "Epoch 24/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8888 - accuracy: 0.1776\n",
            "Epoch 25/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.8382 - accuracy: 0.1801\n",
            "Epoch 26/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7856 - accuracy: 0.2038\n",
            "Epoch 27/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.7335 - accuracy: 0.2003\n",
            "Epoch 28/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.6628 - accuracy: 0.2321\n",
            "Epoch 29/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5979 - accuracy: 0.2341\n",
            "Epoch 30/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.5307 - accuracy: 0.2598\n",
            "Epoch 31/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 3.4704 - accuracy: 0.2689\n",
            "Epoch 32/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.4151 - accuracy: 0.2770\n",
            "Epoch 33/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3746 - accuracy: 0.2851\n",
            "Epoch 34/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3170 - accuracy: 0.3012\n",
            "Epoch 35/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2631 - accuracy: 0.3209\n",
            "Epoch 36/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.2142 - accuracy: 0.3300\n",
            "Epoch 37/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1693 - accuracy: 0.3406\n",
            "Epoch 38/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.1108 - accuracy: 0.3471\n",
            "Epoch 39/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.0700 - accuracy: 0.3547\n",
            "Epoch 40/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 3.0205 - accuracy: 0.3724\n",
            "Epoch 41/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9702 - accuracy: 0.3794\n",
            "Epoch 42/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9636 - accuracy: 0.3744\n",
            "Epoch 43/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.9020 - accuracy: 0.3925\n",
            "Epoch 44/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.8267 - accuracy: 0.4107\n",
            "Epoch 45/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.8187 - accuracy: 0.4021\n",
            "Epoch 46/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7774 - accuracy: 0.4077\n",
            "Epoch 47/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6925 - accuracy: 0.4294\n",
            "Epoch 48/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.6302 - accuracy: 0.4425\n",
            "Epoch 49/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5805 - accuracy: 0.4435\n",
            "Epoch 50/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.5372 - accuracy: 0.4682\n",
            "Epoch 51/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4881 - accuracy: 0.4778\n",
            "Epoch 52/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4498 - accuracy: 0.4808\n",
            "Epoch 53/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.4039 - accuracy: 0.4990\n",
            "Epoch 54/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3557 - accuracy: 0.5081\n",
            "Epoch 55/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3262 - accuracy: 0.5141\n",
            "Epoch 56/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.3160 - accuracy: 0.5086\n",
            "Epoch 57/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.2514 - accuracy: 0.5247\n",
            "Epoch 58/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1994 - accuracy: 0.5399\n",
            "Epoch 59/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1625 - accuracy: 0.5560\n",
            "Epoch 60/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1267 - accuracy: 0.5590\n",
            "Epoch 61/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0839 - accuracy: 0.5610\n",
            "Epoch 62/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0451 - accuracy: 0.5742\n",
            "Epoch 63/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0289 - accuracy: 0.5757\n",
            "Epoch 64/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 2.0217 - accuracy: 0.5691\n",
            "Epoch 65/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.9884 - accuracy: 0.5762\n",
            "Epoch 66/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9417 - accuracy: 0.5848\n",
            "Epoch 67/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9026 - accuracy: 0.5908\n",
            "Epoch 68/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8728 - accuracy: 0.5933\n",
            "Epoch 69/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.8907 - accuracy: 0.5883\n",
            "Epoch 70/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.1114 - accuracy: 0.5373\n",
            "Epoch 71/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.9118 - accuracy: 0.5767\n",
            "Epoch 72/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.8059 - accuracy: 0.5984\n",
            "Epoch 73/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7753 - accuracy: 0.6100\n",
            "Epoch 74/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.7273 - accuracy: 0.6211\n",
            "Epoch 75/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6928 - accuracy: 0.6231\n",
            "Epoch 76/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6658 - accuracy: 0.6302\n",
            "Epoch 77/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 1.6375 - accuracy: 0.6352\n",
            "Epoch 78/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6012 - accuracy: 0.6478\n",
            "Epoch 79/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5771 - accuracy: 0.6574\n",
            "Epoch 80/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5538 - accuracy: 0.6569\n",
            "Epoch 81/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5343 - accuracy: 0.6620\n",
            "Epoch 82/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.5207 - accuracy: 0.6589\n",
            "Epoch 83/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4960 - accuracy: 0.6690\n",
            "Epoch 84/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4764 - accuracy: 0.6665\n",
            "Epoch 85/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4398 - accuracy: 0.6806\n",
            "Epoch 86/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4684 - accuracy: 0.6741\n",
            "Epoch 87/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.4182 - accuracy: 0.6816\n",
            "Epoch 88/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 1.3762 - accuracy: 0.6993\n",
            "Epoch 89/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3560 - accuracy: 0.6998\n",
            "Epoch 90/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3370 - accuracy: 0.6978\n",
            "Epoch 91/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3294 - accuracy: 0.7038\n",
            "Epoch 92/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.3089 - accuracy: 0.7038\n",
            "Epoch 93/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2841 - accuracy: 0.7119\n",
            "Epoch 94/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2773 - accuracy: 0.7104\n",
            "Epoch 95/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2495 - accuracy: 0.7235\n",
            "Epoch 96/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2309 - accuracy: 0.7260\n",
            "Epoch 97/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2142 - accuracy: 0.7301\n",
            "Epoch 98/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2436 - accuracy: 0.7200\n",
            "Epoch 99/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.2415 - accuracy: 0.7170\n",
            "Epoch 100/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1909 - accuracy: 0.7265\n",
            "Epoch 101/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1833 - accuracy: 0.7351\n",
            "Epoch 102/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1523 - accuracy: 0.7457\n",
            "Epoch 103/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1680 - accuracy: 0.7366\n",
            "Epoch 104/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1284 - accuracy: 0.7523\n",
            "Epoch 105/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0968 - accuracy: 0.7634\n",
            "Epoch 106/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1173 - accuracy: 0.7533\n",
            "Epoch 107/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0886 - accuracy: 0.7619\n",
            "Epoch 108/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.1135 - accuracy: 0.7548\n",
            "Epoch 109/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 1.0735 - accuracy: 0.7614\n",
            "Epoch 110/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0316 - accuracy: 0.7805\n",
            "Epoch 111/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0806 - accuracy: 0.7619\n",
            "Epoch 112/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0267 - accuracy: 0.7780\n",
            "Epoch 113/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9969 - accuracy: 0.7775\n",
            "Epoch 114/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9800 - accuracy: 0.7841\n",
            "Epoch 115/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9647 - accuracy: 0.7886\n",
            "Epoch 116/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9531 - accuracy: 0.7886\n",
            "Epoch 117/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9859 - accuracy: 0.7704\n",
            "Epoch 118/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9495 - accuracy: 0.7901\n",
            "Epoch 119/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9236 - accuracy: 0.7982\n",
            "Epoch 120/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9136 - accuracy: 0.7967\n",
            "Epoch 121/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.9062 - accuracy: 0.7982\n",
            "Epoch 122/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8933 - accuracy: 0.8002\n",
            "Epoch 123/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8724 - accuracy: 0.8068\n",
            "Epoch 124/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8612 - accuracy: 0.8073\n",
            "Epoch 125/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8524 - accuracy: 0.8138\n",
            "Epoch 126/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8411 - accuracy: 0.8174\n",
            "Epoch 127/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8366 - accuracy: 0.8143\n",
            "Epoch 128/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8228 - accuracy: 0.8179\n",
            "Epoch 129/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8190 - accuracy: 0.8194\n",
            "Epoch 130/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.8012 - accuracy: 0.8219\n",
            "Epoch 131/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7969 - accuracy: 0.8244\n",
            "Epoch 132/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7868 - accuracy: 0.8244\n",
            "Epoch 133/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7823 - accuracy: 0.8254\n",
            "Epoch 134/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7751 - accuracy: 0.8285\n",
            "Epoch 135/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.8135 - accuracy: 0.8169\n",
            "Epoch 136/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8011 - accuracy: 0.8098\n",
            "Epoch 137/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7976 - accuracy: 0.8153\n",
            "Epoch 138/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.8093 - accuracy: 0.8158\n",
            "Epoch 139/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7598 - accuracy: 0.8280\n",
            "Epoch 140/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7320 - accuracy: 0.8315\n",
            "Epoch 141/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7145 - accuracy: 0.8360\n",
            "Epoch 142/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7161 - accuracy: 0.8401\n",
            "Epoch 143/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.7052 - accuracy: 0.8391\n",
            "Epoch 144/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6867 - accuracy: 0.8426\n",
            "Epoch 145/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6793 - accuracy: 0.8456\n",
            "Epoch 146/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6868 - accuracy: 0.8385\n",
            "Epoch 147/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6979 - accuracy: 0.8380\n",
            "Epoch 148/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6742 - accuracy: 0.8461\n",
            "Epoch 149/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6683 - accuracy: 0.8401\n",
            "Epoch 150/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6540 - accuracy: 0.8456\n",
            "Epoch 151/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6422 - accuracy: 0.8507\n",
            "Epoch 152/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6365 - accuracy: 0.8552\n",
            "Epoch 153/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.6257 - accuracy: 0.8522\n",
            "Epoch 154/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6190 - accuracy: 0.8532\n",
            "Epoch 155/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6156 - accuracy: 0.8522\n",
            "Epoch 156/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6160 - accuracy: 0.8517\n",
            "Epoch 157/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6040 - accuracy: 0.8597\n",
            "Epoch 158/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5945 - accuracy: 0.8572\n",
            "Epoch 159/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5943 - accuracy: 0.8582\n",
            "Epoch 160/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6397 - accuracy: 0.8370\n",
            "Epoch 161/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6324 - accuracy: 0.8446\n",
            "Epoch 162/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.6058 - accuracy: 0.8547\n",
            "Epoch 163/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6058 - accuracy: 0.8517\n",
            "Epoch 164/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5975 - accuracy: 0.8547\n",
            "Epoch 165/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5954 - accuracy: 0.8537\n",
            "Epoch 166/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5822 - accuracy: 0.8582\n",
            "Epoch 167/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5662 - accuracy: 0.8618\n",
            "Epoch 168/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5488 - accuracy: 0.8678\n",
            "Epoch 169/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5436 - accuracy: 0.8693\n",
            "Epoch 170/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5459 - accuracy: 0.8718\n",
            "Epoch 171/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5393 - accuracy: 0.8643\n",
            "Epoch 172/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5319 - accuracy: 0.8683\n",
            "Epoch 173/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5240 - accuracy: 0.8749\n",
            "Epoch 174/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5188 - accuracy: 0.8713\n",
            "Epoch 175/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5117 - accuracy: 0.8764\n",
            "Epoch 176/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5254 - accuracy: 0.8729\n",
            "Epoch 177/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5195 - accuracy: 0.8744\n",
            "Epoch 178/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5083 - accuracy: 0.8739\n",
            "Epoch 179/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4990 - accuracy: 0.8799\n",
            "Epoch 180/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4958 - accuracy: 0.8754\n",
            "Epoch 181/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4966 - accuracy: 0.8779\n",
            "Epoch 182/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4900 - accuracy: 0.8749\n",
            "Epoch 183/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4857 - accuracy: 0.8814\n",
            "Epoch 184/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4752 - accuracy: 0.8799\n",
            "Epoch 185/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4697 - accuracy: 0.8784\n",
            "Epoch 186/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4633 - accuracy: 0.8835\n",
            "Epoch 187/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4631 - accuracy: 0.8845\n",
            "Epoch 188/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4692 - accuracy: 0.8809\n",
            "Epoch 189/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4651 - accuracy: 0.8829\n",
            "Epoch 190/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.4569 - accuracy: 0.8840\n",
            "Epoch 191/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4586 - accuracy: 0.8860\n",
            "Epoch 192/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.4687 - accuracy: 0.8799\n",
            "Epoch 193/200\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.5044 - accuracy: 0.8653\n",
            "Epoch 194/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5073 - accuracy: 0.8673\n",
            "Epoch 195/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4611 - accuracy: 0.8835\n",
            "Epoch 196/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4470 - accuracy: 0.8870\n",
            "Epoch 197/200\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.4420 - accuracy: 0.8850\n",
            "Epoch 198/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4320 - accuracy: 0.8905\n",
            "Epoch 199/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4224 - accuracy: 0.8870\n",
            "Epoch 200/200\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4158 - accuracy: 0.8885\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Q8ZrfI3CH-V",
        "colab_type": "text"
      },
      "source": [
        "# View The Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deLHU6HmCHUa",
        "colab_type": "code",
        "outputId": "6a16bca4-4872-4e62-fe52-76799447324b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bsrN8L9CSd5",
        "colab_type": "text"
      },
      "source": [
        "# Generate New Lyrics!\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WkHQXFquCTEX",
        "colab_type": "code",
        "outputId": "9c3f4483-fa85-4992-80ba-8e796aac0171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "seed_text = \"im feeling chills\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "  token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "  token_list = pad_sequences([token_list], maxlen = max_sequence_len-1, padding = 'pre')\n",
        "  predicted = np.argmax(model.predict(token_list), axis = -1)\n",
        "  output_word = \"\"\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted:\n",
        "      output_word = word\n",
        "      break\n",
        "  seed_text += \" \" + output_word\n",
        "print(seed_text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "im feeling chills a you didnt believe what could with what it was id cutting with christ throwing god feel day fine feeling came but blue making never eyes hour past mistake morning making realized know look song heartaches kisses heartaches touch velvet know heartaches throwing late never had think rotten am at kisses knew realized know heartaches feeling kisses heartaches touch velvet sing didnt making never had would around found never am down christ heartaches start mistake rotten never am hoot making never do would weave kisses heartaches rotten realized know gently closed new know must heartaches kisses kisses feeling wanted rotten\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}